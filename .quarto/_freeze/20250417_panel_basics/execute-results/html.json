{
  "hash": "6b62fc6e6269276716e7c30cbb25d1d6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Panel Data Basics\"\nformat: html\nexecute: \n  warning: false\n---\n\n<style>\n.quarto-title .title {\n  text-align: center;\n}\n</style>\n\n<br><br>\n\n## Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(plm)\nlibrary(broom)\n```\n:::\n\n\n\n<br>\n\n\n## Introduction\n\nWe perform a Monte Carlo simulation to demonstrate the efficiency of the random effects estimator compared to the standard OLS estimator in a panel data setting with a random effects error structure.\n\nThe goal is to show how the OLS estimator, which ignores the panel structure, can be inefficient when there are observation-specific unobserved effects that are uncorrelated with the covariates. In contrast, the random effects estimator exploits this structure to improve estimation precision.\n\n\n<br>\n\n\n## Model - Random Effects\nAssume we have the following panel data model:\n\n$$\nY_{it} = \\alpha + X_{it} \\beta + \\eta_i + \\varepsilon_{it}\n$$\n\nwhere:\n\n- $Y_{it}$ is the dependent variable for observation $i$ at time $t$  \n- $X_{it}$ is the independent variable  \n- $\\alpha$ is a constant  \n- $\\beta$ is the coefficient of interest  \n- $\\eta_i \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$ is the observation-specific effect  \n- $\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ is the idiosyncratic error term\n\nA key assumption here is that the observation-specific effect is uncorrelated with the covariates:\n$$\n\\text{Cov}(X_{it}, \\eta_i) = 0\n$$\n\nThis is the identifying assumption that justifies the use of the random effects estimator. It implies that $\\eta_i$ can be treated as part of the error structure and estimated efficiently using quasi-demeaning. If this assumption were violated, random effects would be biased and inconsistent, and a fixed effects estimator would be preferred.\n\n\n<br>\n\n\n## Simulate Data\n\nWe simulate panel data under the assumption that the data-generating process includes a random effect. Specifically:\n\n- We vary the number of individuals $N$ and hold the number of time periods $T$ fixed\n- The covariate $X_{it}$ is iid normal\n- The observation-specific effect $\\eta_{i}$ and the idiosyncratic error term $\\epsilon_{it}$ are both normal with mean 0 and variance 1\n\nThe true parameter values are:\n\n- $\\alpha = 1$\n- $\\beta = 10$\n\nThis setup mimics a typical balanced panel with strictly exogenous covariates and a classical random effects structure.\n\nThe simulation function:\n\n- Simulates data for a given $N$\n- Estimates both OLS and Random Effects models\n- Returns coefficient estimates and confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set random seed\nset.seed(1)\n\n# Parameter Values\nN_values <- c(10, 25, 50, 100, 200, 500, 1000)\nT <- 5\n\nalpha <- 1\nbeta <- 10\n\nsigma_eta <- 1\nsigma_eps <- 1\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# function to simulate and estimate models\nsimulate_and_estimate <- function(N) {\n  \n  id <- rep(1:N, each = T)\n  time <- rep(1:T, times = N)\n  \n  X <- rnorm(N * T)\n  eta <- rep(rnorm(N, 0, sigma_eta), each = T)\n  eps <- rnorm(N * T, 0, sigma_eps)\n  Y <- alpha + beta * X + eta + eps\n  \n  data <- tibble(id = factor(id), time, X, Y)\n  \n  ols <- lm(Y ~ X, data = data)\n  re  <- plm(Y ~ X, data = data, index = c(\"id\", \"time\"), model = \"random\")\n  \n  bind_rows(\n    tidy(ols, conf.int = TRUE) |> mutate(model = \"OLS\", N = N),\n    tidy(re, conf.int = TRUE)  |> mutate(model = \"Random Effects\", N = N)\n  )\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run simulations\nresults <- map_dfr(N_values, simulate_and_estimate)\n```\n:::\n\n\n\n<br>\n\n\n## Results\n\nOLS ignores the panel structure entirely and treats all $N \\times T$ observations as independent.\n\nRandom Effects correctly accounts for the within-observation correlation introduced by $\\eta_i$, improving efficiency by reducing variance in the estimated coefficients.\n\nThe figure below plots the estimated coefficient on $X$ across different values of $N$. The dashed line marks the true value $\\beta = 10$.\n\nKey points:\n\n- Both OLS and Random Effects are unbiased (on average they estimate the true coefficient)\n- However, Random Effects is more efficient: its confidence intervals are narrower, especially at low $N$\n- As $N$ increases, the precision of both estimators improves, but Random Effects consistently dominates OLS in terms of standard error\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults %>% \n  filter(term==\"X\") %>% \n  ggplot(aes(x = N, y = estimate, color = model)) +\n  geom_point() +\n  geom_errorbar(\n    aes(ymin = conf.low, ymax = conf.high),\n    width = 0.1\n  ) +\n  scale_x_continuous(trans = \"log10\", breaks = N_values) +\n  geom_hline(yintercept = beta, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Coefficient Estimates with 95% Confidence Intervals\",\n    subtitle = \"Dashed line indicates true value of β = 10\",\n    y = \"Estimated Coefficient\", x = \"Number of Individuals (N)\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](20250417_panel_basics_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe figure below plots the estimated coefficient on $\\alpha$ across different values of $N$. The dashed line marks the true value $\\alpha = 1$.\n\nKey points:\n\n- Both OLS and Random Effects are unbiased (on average they estimate the true coefficient)\n- However, Random Effects is less efficient: its confidence intervals are larger, especially at low $N$\n- As $N$ increases, the precision of both estimators improves, but OLS consistently dominates Random Effects in terms of standard error\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults %>% \n  filter(term==\"(Intercept)\") %>% \n  ggplot(aes(x = N, y = estimate, color = model)) +\n  geom_point() +\n  geom_errorbar(\n    aes(ymin = conf.low, ymax = conf.high),\n    width = 0.1\n  ) +\n  scale_x_continuous(trans = \"log10\", breaks = N_values) +\n  geom_hline(yintercept = alpha, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Coefficient Estimates with 95% Confidence Intervals\",\n    subtitle = \"Dashed line indicates true value of α = 1\",\n    y = \"Estimated Coefficient\", x = \"Number of Individuals (N)\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](20250417_panel_basics_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n<br>\n\n\n## Summary\n\nEfficiency gains from Random Effects are largest when $N$ is small and apply to the slope coefficient.\n\nIn real-world applications, where observation-level heterogeneity is present and uncorrelated with the covariates, Random Effects can provide more precise inference than pooled OLS.\n\n\n\n\n\n\n\n",
    "supporting": [
      "20250417_panel_basics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}