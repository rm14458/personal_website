[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is Robert Martin. I work as a data scientist and this is my personal website. It is mostly just a collection of useful results in different areas of statistics/data science."
  },
  {
    "objectID": "6_energy_demand_forecasting/energy_demand_forecasting.html",
    "href": "6_energy_demand_forecasting/energy_demand_forecasting.html",
    "title": "About",
    "section": "",
    "text": "This page is work in progress."
  },
  {
    "objectID": "4_decision_trees/decision_trees.html",
    "href": "4_decision_trees/decision_trees.html",
    "title": "About",
    "section": "",
    "text": "This page is work in progress."
  },
  {
    "objectID": "3_panel_data/20250424_re_vs_fe.html#packages",
    "href": "3_panel_data/20250424_re_vs_fe.html#packages",
    "title": "Random Effects vs Fixed Effects",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(plm)\nlibrary(broom)"
  },
  {
    "objectID": "3_panel_data/20250424_re_vs_fe.html#introduction",
    "href": "3_panel_data/20250424_re_vs_fe.html#introduction",
    "title": "Random Effects vs Fixed Effects",
    "section": "Introduction",
    "text": "Introduction\nWe conduct a Monte Carlo simulation to compare the performance of OLS, Random Effects, and Fixed Effects estimators in a panel data setting where the unobserved observation-specific effect is correlated with the covariate of interest. This setting violates the key assumption behind the Random Effects estimator, and we demonstrate how this affects bias and precision."
  },
  {
    "objectID": "3_panel_data/20250424_re_vs_fe.html#model---fixed-effects",
    "href": "3_panel_data/20250424_re_vs_fe.html#model---fixed-effects",
    "title": "Random Effects vs Fixed Effects",
    "section": "Model - Fixed Effects",
    "text": "Model - Fixed Effects\nAssume we have the following panel data model:\n\\[\nY_{it} = \\alpha + X_{it} \\beta + \\eta_i + \\varepsilon_{it}\n\\]\nwhere:\n\n\\(Y_{it}\\) is the dependent variable for observation \\(i\\) at time \\(t\\)\n\n\\(X_{it}\\) is the independent variable\n\n\\(\\alpha\\) is a constant\n\n\\(\\beta\\) is the coefficient we want to estimate\n\n\\(\\eta_i \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\\) is an unobserved observation-specific effect\n\\(\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)\\) is the idiosyncratic error term\n\nIn this simulation, we violate the Random Effects assumption by allowing: \\[\n\\text{Cov}(X_{it}, \\eta_i) \\neq 0\n\\]\nSpecifically, we generate \\(X_{it}\\) as a function of \\(\\eta_i\\) to induce correlation. This ensures that both OLS and RE estimators will be biased. In contrast, the Fixed Effects estimator removes \\(\\eta_i\\) via within-transformation, yielding consistent estimates of \\(\\beta\\)."
  },
  {
    "objectID": "3_panel_data/20250424_re_vs_fe.html#simulate-data",
    "href": "3_panel_data/20250424_re_vs_fe.html#simulate-data",
    "title": "Random Effects vs Fixed Effects",
    "section": "Simulate Data",
    "text": "Simulate Data\nWe simulate panel data under the assumption that the data-generating process includes a fixed effect that is correlated with the covariate. Specifically:\n\nWe vary the number of observations \\(N\\) and hold the number of time periods \\(T\\) fixed\nThe individual-specific effect \\(\\eta_i\\) is drawn from a normal distribution and held constant across time\nThe covariate \\(X_{it}\\) is generated as a function of \\(\\eta_i\\), inducing correlation between \\(X_{it}\\) and the unobserved effect\nThe idiosyncratic error term \\(\\varepsilon_{it}\\) is iid normal with mean 0 and variance 1\n\nThe true parameter values are:\n\n\\(\\alpha = 1\\)\n\\(\\beta = 10\\)\n\nThis setup mimics a balanced panel where the regressor is endogenous with respect to the observation-specific effect, violating the assumptions behind the Random Effects estimator. As a result, OLS and RE will be biased, and only the Fixed Effects estimator will be consistent.\nThe simulation function:\n\nSimulates data for a given \\(N\\)\nEstimates OLS, Random Effects, and Fixed Effects models\nReturns coefficient estimates and confidence intervals\n\n\n# random seed\nset.seed(1)\n\n# Parameter Values\nN_values &lt;- c(10, 25, 50, 100, 200, 500, 1000)\nT &lt;- 5\n\nalpha &lt;- 1\nbeta &lt;- 10\n\nsigma_eta &lt;- 1\nsigma_eps &lt;- 1\n\n\n# Function to simulate and estimate models\nsimulate_and_estimate &lt;- function(N) {\n  \n  id &lt;- rep(1:N, each = T)\n  time &lt;- rep(1:T, times = N)\n  \n  eta_i &lt;- rnorm(N, 0, sigma_eta)\n  eta &lt;- rep(eta_i, each = T)\n  \n  # Induce correlation: X depends on eta_i\n  X &lt;- rep(0, N * T)\n  for (i in 1:N) {\n    X[((i - 1) * T + 1):(i * T)] &lt;- 0.5 * eta_i[i] + rnorm(T)\n  }\n  \n  eps &lt;- rnorm(N * T, 0, sigma_eps)\n  Y &lt;- alpha + beta * X + eta + eps\n  \n  data &lt;- tibble(id = factor(id), time, X, Y)\n  \n  ols &lt;- lm(Y ~ X, data = data)\n  re  &lt;- plm(Y ~ X, data = data, index = c(\"id\", \"time\"), model = \"random\")\n  fe  &lt;- plm(Y ~ X, data = data, index = c(\"id\", \"time\"), model = \"within\")\n  \n  bind_rows(\n    tidy(ols, conf.int = TRUE) |&gt; mutate(model = \"OLS\", N = N),\n    tidy(re, conf.int = TRUE)  |&gt; mutate(model = \"Random Effects\", N = N),\n    tidy(fe, conf.int = TRUE)  |&gt; mutate(model = \"Fixed Effects\", N = N)\n  )\n\n}\n\n\n# run simulations\nresults &lt;- map_dfr(N_values, simulate_and_estimate)"
  },
  {
    "objectID": "3_panel_data/20250424_re_vs_fe.html#results",
    "href": "3_panel_data/20250424_re_vs_fe.html#results",
    "title": "Random Effects vs Fixed Effects",
    "section": "Results",
    "text": "Results\nThe plot below shows the estimated coefficient on \\(X\\) across different values of \\(N\\). The dashed horizontal line marks the true value \\(\\beta = 10\\).\nKey points:\n\nOnly the Fixed Effects estimator is unbiased: its estimates center around the true value across all \\(N\\)\nOLS and Random Effects are biased: they consistently overestimate \\(\\beta\\) due to the positive correlation between \\(X_{it}\\) and \\(\\eta_i\\)\nAs \\(N\\) increases, the bias in OLS and RE does not vanish\nThe Fixed Effects estimator becomes more precise (narrower confidence intervals) as \\(N\\) increases, as expected due to larger sample size\n\nThis simulation confirms that when \\(\\eta_i\\) is correlated with \\(X_{it}\\), only the Fixed Effects estimator yields consistent estimates. OLS and RE fail because they attribute part of the variation in \\(Y_{it}\\) caused by \\(\\eta_i\\) to \\(X_{it}\\).\n\nresults %&gt;% \n  filter(term == \"X\") %&gt;% \n  ggplot(aes(x = N, y = estimate, color = model)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1) +\n  scale_x_continuous(trans = \"log10\", breaks = N_values) +\n  geom_hline(yintercept = beta, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Coefficient Estimates for X\",\n    subtitle = \"Dashed line shows true β = 10. Only FE gives unbiased estimate.\",\n    y = \"Estimated Coefficient\", x = \"Number of Individuals (N)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot below shows the estimated intercept \\(\\alpha\\) from the OLS and Random Effects models. The true value is \\(\\alpha = 1\\) (dashed line). The Fixed Effects model is excluded because it does not estimate the overall intercept — it only recovers within-observation deviations from observation means.\nKey points:\n\nBoth OLS and RE recover \\(\\alpha\\) with similar levels of bias and precision\nAs \\(N\\) increases, the estimates of \\(\\alpha\\) become more precise for both models\nThe Fixed Effects estimator intentionally omits \\(\\alpha\\) because the within transformation sweeps it out\n\nThis illustrates that while FE is superior for estimating slope coefficients under correlation between \\(\\eta_i\\) and \\(X_{it}\\), it does not provide information about the level of \\(\\alpha\\) or other time-invariant effects.\n\nresults %&gt;% \n  filter(term == \"(Intercept)\", model != \"Fixed Effects\") %&gt;% \n  ggplot(aes(x = N, y = estimate, color = model)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1) +\n  scale_x_continuous(trans = \"log10\", breaks = N_values) +\n  geom_hline(yintercept = alpha, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Intercept Estimates (α)\",\n    subtitle = \"Dashed line shows true α = 1. FE does not estimate intercept.\",\n    y = \"Estimated Intercept\", x = \"Number of Individuals (N)\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "3_panel_data/20250424_re_vs_fe.html#summary",
    "href": "3_panel_data/20250424_re_vs_fe.html#summary",
    "title": "Random Effects vs Fixed Effects",
    "section": "Summary",
    "text": "Summary\n\nWhen observation-specific effects are correlated with regressors, the Fixed Effects estimator is consistent, while OLS and Random Effects are biased\nRandom Effects gains in efficiency only apply if the unobserved effect is uncorrelated with covariates — that assumption is violated here\nFixed Effects does not estimate the intercept — this is a feature, not a bug: it focuses on within-observation variation\nAs sample size increases, FE estimates become more precise, while OLS and RE remain biased"
  },
  {
    "objectID": "2_time_series/time_series.html",
    "href": "2_time_series/time_series.html",
    "title": "About",
    "section": "",
    "text": "This page is work in progress."
  },
  {
    "objectID": "1_bayesian_statistics/20250410_multivariate_regression.html",
    "href": "1_bayesian_statistics/20250410_multivariate_regression.html",
    "title": "Multivariate Regression",
    "section": "",
    "text": "Multivariate Regression\n\n\nPackages\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, invgamma, norm\n\n\nnp.random.seed(42)\n\n\n\nModel Specification\nAssume we are considering the following model:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\nWhere:\n\n\\(\\mathbf{Y}\\) is the \\(N \\times 1\\) response vector\n\\(\\mathbf{X}\\) is the \\(N \\times P\\) design matrix (each row represents one observation)\n\\(\\boldsymbol{\\beta}\\) is the \\(P \\times 1\\) vector of regression coefficients\n\\(\\boldsymbol{\\epsilon}\\) is the \\(N \\times 1\\) vector of errors\n\nThe error term is distributed as:\n\\[\n\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{I}_N)\n\\]\nWhere:\n\n\\(\\mathbf{0}\\) is an \\(N \\times 1\\) vector of zeros\n\\(\\mathbf{I}_N\\) is the \\(N \\times N\\) identity matrix\n\\(\\sigma^2\\) is the common variance parameter\n\n\n\nVisualise Simulated Data\n\n# parameter values\nN = 100 # n observations\nP = 3    # n of predictors\ntrue_beta = np.array([1.5, -2.0, 0.5])  # true coefficients\ntrue_sigma = 0.5\ntrue_sigma_sq = true_sigma**2\n\n# generate simulated data\nX = np.random.randn(N, P)  # design matrix with N observations and P predictors\nepsilon = np.random.randn(N) * true_sigma  # errors\ny = X @ true_beta + epsilon  # response variable\n\n\n# Create a figure with 3 subplots (one for each predictor)\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Loop through each predictor\nfor i in range(P):\n    ax = axes[i]\n    \n    # Plot raw data points for this predictor\n    ax.scatter(X[:, i], y, alpha=0.5, label='Observed Data', color='blue')\n    \n    # Create prediction matrix where we vary only the current predictor\n    x_line = np.linspace(min(X[:, i]), max(X[:, i]), 100)\n    X_line = np.zeros((100, P))\n    X_line[:, i] = x_line  # vary only the i-th predictor\n    \n    # Calculate conditional expectation (other predictors set to 0)\n    y_line = X_line @ true_beta\n    \n    # Plot true conditional relationship\n    ax.plot(x_line, y_line, 'r-', linewidth=2,\n            label=f'E[y|x{i+1}] = {true_beta[i]:.2f}x{i+1} + ' + \n                  ' + '.join([f'{true_beta[j]:.2f}E[x{j+1}]' for j in range(P) if j != i]))\n    \n    # Customize subplot\n    ax.set_title(f'Relationship with x{i+1}', fontsize=14)\n    ax.set_xlabel(f'x{i+1}', fontsize=12)\n    ax.set_ylabel('y', fontsize=12)\n    ax.grid(True, linestyle='--', alpha=0.3)\n    ax.legend(fontsize=9, loc='upper left' if i == 0 else 'best')\n\nplt.suptitle('Multivariate Regression: Conditional Relationships with Each Predictor\\n(Other predictors fixed at E[x]=0)', \n             fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nConditional Distribution\nFor each observation \\(i\\):\n\\[\ny_i | \\boldsymbol{\\beta}, \\sigma^2 \\sim \\mathcal{N}(\\mathbf{x}_i\\boldsymbol{\\beta}, \\sigma^2)\n\\]\nWhere:\n\n\\(y_i\\) is the scalar response for observation \\(i\\)\n\\(\\mathbf{x}_i\\) is the \\(1 \\times P\\) row vector of predictors for observation \\(i\\)\n\\(\\boldsymbol{\\beta}\\) is the \\(P \\times 1\\) column vector of coefficients\n\\(\\sigma^2\\) is the scalar variance parameter\n\nGiven \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), and \\(\\sigma^2\\), the likelihood of \\(\\mathbf{Y}\\) is:\n\\[\np(\\mathbf{Y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) = (2\\pi\\sigma^2)^{-N/2} \\exp\\left(-\\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)\n\\]\n\n\nPrior Distributions\nWe assume the following prior for \\(\\boldsymbol{\\beta}\\) given \\(\\sigma^2\\) (multivariate normal):\n\\[\np(\\boldsymbol{\\beta} | \\sigma^2) = (2\\pi\\sigma^2)^{-P/2} |\\mathbf{A}_0|^{1/2} \\exp\\left(-\\frac{1}{2\\sigma^2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu}_0)^\\top \\mathbf{A}_0 (\\boldsymbol{\\beta} - \\boldsymbol{\\mu}_0)\\right)\n\\]\nParameters:\n\n\\(\\boldsymbol{\\mu}_0\\): \\(P \\times 1\\) prior mean vector\n\\(\\mathbf{A}_0\\): \\(P \\times P\\) prior precision matrix\n\\(\\sigma^2\\): Scales the covariance matrix (\\(\\sigma^2 \\mathbf{A}_0^{-1}\\))\n\nWe assume the following prior for \\(\\sigma^2\\) (Inverse Gamma):\n\\[\np(\\sigma^2) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} (\\sigma^2)^{-(a_0 + 1)} \\exp\\left(-\\frac{b_0}{\\sigma^2}\\right)\n\\]\nParameters:\n\n\\(a_0 &gt; 0\\): Shape parameter\n\\(b_0 &gt; 0\\): Scale parameter\n\n\n# Prior parameters (multivariate normal for beta)\nP = 3  # number of predictors\nmu_0 = np.zeros(P)  # prior mean vector\nA_0 = np.eye(P)  # prior precision matrix (identity)\n\n# Prior parameters (inverse gamma for sigma^2)\na_0 = 2.5  # shape parameter\nb_0 = 1.5  # scale parameter\n\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# =============================================\n# Plot 1: Prior for beta (multivariate normal)\n# =============================================\n\n# We'll show marginal distributions for each beta_p\nbeta_grid = np.linspace(-3, 3, 500)\ncolors = ['blue', 'green', 'red']\n\nfor p in range(P):\n    # Marginal distribution for beta_p ~ N(mu_0[p], sigma^2 * (A_0^{-1}[p,p]))\n    # Here we assume sigma^2 = 1 for visualization (since it's conditional)\n    marginal_std = np.sqrt(np.linalg.inv(A_0)[p,p])  # sqrt of diagonal element\n    prior_density = norm.pdf(beta_grid, loc=mu_0[p], scale=marginal_std)\n    \n    ax1.plot(beta_grid, prior_density, '-', linewidth=2, color=colors[p],\n             label=rf'$\\beta_{p+1} \\sim \\mathcal{{N}}(0, {marginal_std**2:.2f})$')\n\n# Customize beta prior plot\nax1.set_title('Prior Distributions for β Components\\n(Conditional on σ²=1)', fontsize=14)\nax1.set_xlabel('β values', fontsize=12)\nax1.set_ylabel('Probability Density', fontsize=12)\nax1.legend(fontsize=10)\nax1.grid(True, linestyle='--', alpha=0.3)\n\n# =============================================\n# Plot 2: Prior for sigma^2 (inverse gamma)\n# =============================================\n\n# Create grid of sigma^2 values\nsigma_sq_grid = np.linspace(0.01, 5, 500)\nprior_sigma_sq = invgamma.pdf(sigma_sq_grid, a=a_0, scale=b_0)\n\nax2.plot(sigma_sq_grid, prior_sigma_sq, 'm-', linewidth=2,\n         label=rf'$\\sigma^2 \\sim \\mathrm{{Inv-Gamma}}({a_0}, {b_0})$')\n\n# Highlight mode (a_0 &gt; 1) and mean\nif a_0 &gt; 1:\n    mode = b_0 / (a_0 + 1)\n    ax2.axvline(mode, color='k', linestyle='--', alpha=0.5,\n                label=f'Mode = {mode:.2f}')\nmean = b_0 / (a_0 - 1) if a_0 &gt; 1 else np.inf\nax2.axvline(mean, color='r', linestyle=':', alpha=0.5,\n            label=f'Mean = {mean:.2f}' if np.isfinite(mean) else 'Mean = ∞')\n\n# Customize sigma^2 prior plot\nax2.set_title('Prior Distribution for σ²', fontsize=14)\nax2.set_xlabel('σ² (variance)', fontsize=12)\nax2.set_ylabel('Probability Density', fontsize=12)\nax2.legend(fontsize=10)\nax2.grid(True, linestyle='--', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nJoint Posterior Distribution\nThe joint posterior distribution is proportional to:\n\\[\np(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{Y}, \\mathbf{X}) \\propto p(\\mathbf{Y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) \\cdot p(\\boldsymbol{\\beta} | \\sigma^2) \\cdot p(\\sigma^2)\n\\]\nIt can be shown that the conditional posterior for \\(\\boldsymbol{\\beta}\\) is:\n\\[\n\\boldsymbol{\\beta} | \\sigma^2, \\mathbf{Y}, \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_N, \\sigma^2 \\mathbf{A}_N^{-1})\n\\]\nWhere the posterior parameters are: \\[\n\\begin{aligned}\n\\mathbf{A}_N &= \\mathbf{X}^\\top\\mathbf{X} + \\mathbf{A}_0 \\\\\n\\boldsymbol{\\mu}_N &= \\mathbf{A}_N^{-1}(\\mathbf{X}^\\top\\mathbf{Y} + \\mathbf{A}_0\\boldsymbol{\\mu}_0)\n\\end{aligned}\n\\]\nLikewise it can be shown that the marginal posterior for \\(\\sigma^2\\) is:\n\\[\n\\sigma^2 | \\mathbf{Y}, \\mathbf{X} \\sim \\text{Inv-Gamma}(a_N, b_N)\n\\]\nWhere the posterior parameters are: \\[\n\\begin{aligned}\na_N &= a_0 + \\frac{N}{2} \\\\\nb_N &= b_0 + \\frac{1}{2}\\left(\\mathbf{Y}^\\top\\mathbf{Y} + \\boldsymbol{\\mu}_0^\\top\\mathbf{A}_0\\boldsymbol{\\mu}_0 - \\boldsymbol{\\mu}_N^\\top\\mathbf{A}_N\\boldsymbol{\\mu}_N\\right)\n\\end{aligned}\n\\]\n\n# Observation subsets to plot\nobservation_subsets = [5, 50, N]\ncolors = ['green', 'orange', 'red']\nline_styles = [':', '--', '-']\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n\n# =============================================\n# Plot 1: Posterior for β components\n# =============================================\nbeta_grid = np.linspace(-3, 3, 500)\n\n# Plot prior (marginal for first β component)\nprior_std = np.sqrt(np.linalg.inv(A_0)[0,0])\nax1.plot(beta_grid, norm.pdf(beta_grid, mu_0[0], prior_std), \n         'b-', linewidth=3, label='Prior')\n\nfor n_obs, color, ls in zip(observation_subsets, colors, line_styles):\n    X_sub = X[:n_obs]\n    y_sub = y[:n_obs]\n    \n    # Calculate posterior parameters for β\n    A_n = X_sub.T @ X_sub + A_0\n    mu_n = np.linalg.solve(A_n, X_sub.T @ y_sub + A_0 @ mu_0)\n    \n    # For visualization, we'll show marginal for β1\n    post_var = np.linalg.inv(A_n)[0,0]  # Assuming σ²=1 for visualization\n    post_std = np.sqrt(post_var)\n    \n    ax1.plot(beta_grid, norm.pdf(beta_grid, mu_n[0], post_std),\n             color=color, linestyle=ls, linewidth=2,\n             label=f'Posterior (n={n_obs})')\n\nax1.axvline(true_beta[0], color='k', linestyle='--', label='True β₁')\nax1.set_title('Posterior Distribution for β₁', fontsize=14)\nax1.set_xlabel('β₁ value', fontsize=12)\nax1.set_ylabel('Density', fontsize=12)\nax1.legend(fontsize=10)\nax1.grid(True, linestyle='--', alpha=0.3)\n\n# =============================================\n# Plot 2: Posterior for σ²\n# =============================================\nsigma_sq_grid = np.linspace(0.01, 1.5, 500)\n\n# Plot prior\nax2.plot(sigma_sq_grid, invgamma.pdf(sigma_sq_grid, a=a_0, scale=b_0),\n         'b-', linewidth=3, label='Prior')\n\nfor n_obs, color, ls in zip(observation_subsets, colors, line_styles):\n    X_sub = X[:n_obs]\n    y_sub = y[:n_obs]\n    \n    # Calculate posterior parameters for σ²\n    A_n = X_sub.T @ X_sub + A_0\n    mu_n = np.linalg.solve(A_n, X_sub.T @ y_sub + A_0 @ mu_0)\n    \n    a_n = a_0 + n_obs/2\n    b_n = b_0 + 0.5*(y_sub.T @ y_sub + mu_0.T @ A_0 @ mu_0 - mu_n.T @ A_n @ mu_n)\n    \n    ax2.plot(sigma_sq_grid, invgamma.pdf(sigma_sq_grid, a=a_n, scale=b_n),\n             color=color, linestyle=ls, linewidth=2,\n             label=f'Posterior (n={n_obs})')\n\nax2.axvline(true_sigma_sq, color='k', linestyle='--', label='True σ²')\nax2.set_title('Posterior Distribution for σ²', fontsize=14)\nax2.set_xlabel('σ² value', fontsize=12)\nax2.set_ylabel('Density', fontsize=12)\nax2.legend(fontsize=10)\nax2.grid(True, linestyle='--', alpha=0.3)\n\nplt.suptitle('Evolution of Posterior Distributions with Increasing Data', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSampling From The Posteriors\n\n# Log-posterior function (up to a constant)\ndef log_posterior(beta, sigma_sq, X, y, mu_0, A_0, a_0, b_0):\n    # Log likelihood\n    residuals = y - X @ beta\n    log_likelihood = -0.5 * N * np.log(sigma_sq) - 0.5 * np.sum(residuals**2) / sigma_sq\n    \n    # Log prior for beta\n    beta_diff = beta - mu_0\n    log_prior_beta = -0.5 * beta_diff.T @ A_0 @ beta_diff / sigma_sq\n    \n    # Log prior for sigma_sq\n    log_prior_sigma = -(a_0 + 1) * np.log(sigma_sq) - b_0 / sigma_sq\n    \n    return log_likelihood + log_prior_beta + log_prior_sigma\n\n\n# Metropolis-Hastings sampler\ndef metropolis_hastings(X, y, initial_beta, initial_sigma_sq, \n                        mu_0, A_0, a_0, b_0, \n                        n_samples=5000, beta_step=0.1, sigma_step=0.1):\n    \n    # Initialize storage\n    beta_samples = np.zeros((n_samples, P))\n    sigma_sq_samples = np.zeros(n_samples)\n    \n    current_beta = initial_beta.copy()\n    current_sigma_sq = initial_sigma_sq\n    accepted = 0\n    \n    for i in range(n_samples):\n        # Sample beta\n        proposed_beta = current_beta + np.random.normal(0, beta_step, P)\n        \n        # Compute acceptance ratio for beta\n        log_alpha = (log_posterior(proposed_beta, current_sigma_sq, X, y, mu_0, A_0, a_0, b_0) - \n                    log_posterior(current_beta, current_sigma_sq, X, y, mu_0, A_0, a_0, b_0))\n        \n        if np.log(np.random.rand()) &lt; log_alpha:\n            current_beta = proposed_beta\n            accepted += 1\n        \n        # Sample sigma_sq (using log-normal proposal)\n        proposed_sigma_sq = current_sigma_sq * np.exp(np.random.normal(0, sigma_step))\n        \n        # Compute acceptance ratio for sigma_sq (with Jacobian term)\n        log_alpha = (log_posterior(current_beta, proposed_sigma_sq, X, y, mu_0, A_0, a_0, b_0) - \n                    log_posterior(current_beta, current_sigma_sq, X, y, mu_0, A_0, a_0, b_0) + \n                    np.log(proposed_sigma_sq) - np.log(current_sigma_sq))  # Jacobian adjustment\n        \n        if np.log(np.random.rand()) &lt; log_alpha:\n            current_sigma_sq = proposed_sigma_sq\n            accepted += 1\n        \n        # Store samples\n        beta_samples[i] = current_beta\n        sigma_sq_samples[i] = current_sigma_sq\n    \n    acceptance_rate = accepted / (2 * n_samples)  # Two updates per iteration\n    print(f\"Overall acceptance rate: {acceptance_rate:.2f}\")\n    \n    return beta_samples, sigma_sq_samples\n\n\n# Run MCMC\ninitial_beta = np.zeros(P)\ninitial_sigma_sq = 1.0\nbeta_samples, sigma_sq_samples = metropolis_hastings(\n    X, y, initial_beta, initial_sigma_sq, \n    mu_0, A_0, a_0, b_0,\n    n_samples=10000, beta_step=0.1, sigma_step=0.1\n)\n\n\n# Plot results\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Trace plots for beta components\nfor p in range(P):\n    axes[0,0].plot(beta_samples[:, p], alpha=0.7, \n                   label=f'β{p+1} (True = {true_beta[p]:.1f})')\naxes[0,0].set_title('Trace Plots for β Components')\naxes[0,0].set_xlabel('Iteration')\naxes[0,0].set_ylabel('Parameter Value')\naxes[0,0].legend()\naxes[0,0].grid(True, alpha=0.3)\n\n# Trace plot for sigma²\naxes[0,1].plot(sigma_sq_samples, alpha=0.7, color='purple',\n               label=f'σ² (True = {true_sigma**2:.2f})')\naxes[0,1].set_title('Trace Plot for σ²')\naxes[0,1].set_xlabel('Iteration')\naxes[0,1].set_ylabel('σ² Value')\naxes[0,1].legend()\naxes[0,1].grid(True, alpha=0.3)\n\n# Histograms for beta components\nfor p in range(P):\n    axes[1,0].hist(beta_samples[2000:, p], bins=30, density=True, alpha=0.6,\n                   label=f'β{p+1} (True = {true_beta[p]:.1f})')\naxes[1,0].set_title('Posterior Distributions for β Components')\naxes[1,0].set_xlabel('Parameter Value')\naxes[1,0].set_ylabel('Density')\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# Histogram for sigma²\naxes[1,1].hist(sigma_sq_samples[2000:], bins=30, density=True, alpha=0.6, color='purple',\n               label=f'σ² (True = {true_sigma**2:.2f})')\naxes[1,1].set_title('Posterior Distribution for σ²')\naxes[1,1].set_xlabel('σ² Value')\naxes[1,1].set_ylabel('Density')\naxes[1,1].legend()\naxes[1,1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nOverall acceptance rate: 0.49"
  },
  {
    "objectID": "1_bayesian_statistics/20250414_coin_flipping.html",
    "href": "1_bayesian_statistics/20250414_coin_flipping.html",
    "title": "Coin Flipping",
    "section": "",
    "text": "Coin Flipping\n\n\nPackages\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, beta\n\n\nnp.random.seed(42)\n\n\n\nIntroduction\nIn this notebook, we study a simple probabilistic model for coin flipping. We assume each flip results in either a head (1) or tail (0), modeled as a Bernoulli random variable with unknown success probability \\(\\theta\\). Our goals are to:\n\nSimulate coin flips under a known value of \\(\\theta\\)\nSpecify the likelihood function for a sequence of flips\nVisualize simulated outcomes and their empirical properties\nUse the likelihood to explore inference on \\(\\theta\\)\n\nThis example provides an intuitive foundation for understanding Bayesian updating in discrete probability models.\n\n\nModel Specification\nLet \\(y_i\\) be the result of flipping a coin which lands on heads with probability \\(\\theta\\) and tails with probability \\(1-\\theta\\). If it lands on heads we set \\(y_i = 1\\) and if it lands on tails we set \\(y_i = 0\\). We therefore have that each observation \\(y_i\\) follows a Bernoulli distribution:\n\\[\n  y_i \\mid \\theta \\sim \\text{Bernoulli}(\\theta), \\quad i = 1, \\dots, N,\n  \\]\nwhere \\(\\theta\\) is the probability of success.\nGiven \\(N\\) i.i.d. observations \\(\\mathbf{y} = (y_1, \\dots, y_N)\\), the joint likelihood is:\n\\[\np(\\mathbf{y} \\mid \\theta) = \\prod_{i=1}^N p(y_i \\mid \\theta) = \\prod_{i=1}^N \\theta^{y_i} (1 - \\theta)^{1 - y_i}.\n\\]\nLet \\(S = \\sum_{i=1}^N y_i\\) (total successes). The likelihood becomes:\n\\[\np(\\mathbf{y} \\mid \\theta) = \\theta^S (1 - \\theta)^{N - S}.\n\\]\n\n\nVisualise Simulated Data\n\n# Parameter values\nN = 500                 # Number of observations\ntrue_theta = 0.3        # True probability of success\n\n# Generate simulated Bernoulli data\ny = np.random.binomial(n=1, p=true_theta, size=N)\n\n# Calculate cumulative success rate\ncumulative_success = np.cumsum(y)\ncumulative_rate = cumulative_success / (np.arange(N) + 1)\n\n\n# Create plot\nplt.figure(figsize=(12, 6))\n\n# Plot raw binary data (jittered for visibility)\nplt.scatter(np.arange(N), y + np.random.uniform(-0.05, 0.05, N), \n            alpha=0.3, label='Individual observations (jittered)', color='blue')\n\n# Plot cumulative success rate\nplt.plot(cumulative_rate, 'r-', linewidth=2, \n         label=f'Cumulative success rate (θ={true_theta})')\n\n# Add horizontal lines for reference\nplt.axhline(y=true_theta, color='green', linestyle='--', \n            label=f'True probability (θ={true_theta})')\nplt.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n\n# Customize plot\nplt.title('Simulated Bernoulli Data and Cumulative Success Rate', fontsize=14)\nplt.xlabel('Observation number', fontsize=12)\nplt.ylabel('Success (1) / Failure (0)', fontsize=12)\nplt.ylim(-0.1, 1.1)\nplt.yticks([0, 0.5, 1])\nplt.grid(True, linestyle='--', alpha=0.3)\nplt.legend(fontsize=10, loc='upper right')\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPrior Distribution\nWe assume a Beta prior for \\(\\theta\\):\n\\[\n\\theta \\sim \\text{Beta}(\\alpha, \\beta),\n\\]\nwith hyperparameters \\(\\alpha, \\beta &gt; 0\\).\n\n# Prior parameters (Beta distribution)\nalpha_0 = 2    # Prior \"successes\" + 1\nbeta_0 = 2     # Prior \"failures\" + 1\n\n\n# Create grid of theta values\ntheta_grid = np.linspace(0, 1, 500)\n\n# Calculate prior density\nprior_density = beta.pdf(theta_grid, a=alpha_0, b=beta_0)\n\n# Create plot\nplt.figure(figsize=(10, 6))\nplt.plot(theta_grid, prior_density, 'b-', linewidth=2, \n         label=f'Prior: Beta(α={alpha_0}, β={beta_0})')\n\n# Highlight mean and 95% credible interval\nmean = alpha_0 / (alpha_0 + beta_0)\nci_low, ci_high = beta.ppf([0.025, 0.975], a=alpha_0, b=beta_0)\n\nplt.axvline(mean, color='k', linestyle='--', alpha=0.5, \n            label=f'Mean = {mean:.2f}')\nplt.axvline(ci_low, color='r', linestyle=':', alpha=0.5)\nplt.axvline(ci_high, color='r', linestyle=':', alpha=0.5,\n            label='95% credible interval')\n\n# Customize plot\nplt.title('Prior Distribution for θ', fontsize=14)\nplt.xlabel('θ (probability of success)', fontsize=12)\nplt.ylabel('Probability Density', fontsize=12)\nplt.legend(fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPosterior Distribution\nThe posterior is proportional to:\n\\[\np(\\theta \\mid \\mathbf{y}) \\propto \\theta^S (1 - \\theta)^{N - S} \\cdot \\theta^{\\alpha-1} (1 - \\theta)^{\\beta-1} = \\theta^{\\alpha + S - 1} (1 - \\theta)^{\\beta + N - S - 1}.\n\\]\nThis is a Beta distribution:\n\\[\n\\theta \\mid \\mathbf{y} \\sim \\text{Beta}(\\alpha + S, \\beta + N - S).\n\\]\n\n# Define observation subsets\nobservation_subsets = [10, 50, 200, 500]  # Different sample sizes to plot\ncolors = ['green', 'orange', 'red', 'purple']\nline_styles = [':', '--', '-.', '-']\n\n# Create grid of theta values\ntheta_grid = np.linspace(0, 1, 500)\n\n# Create plot\nplt.figure(figsize=(12, 7))\n\n# Plot prior\nprior_density = beta.pdf(theta_grid, alpha_0, beta_0)\nplt.plot(theta_grid, prior_density, 'b-', linewidth=3, \n         label=f'Prior: Beta(α={alpha_0}, β={beta_0})')\n\n# Plot posteriors for different observation counts\nfor n_obs, color, ls in zip(observation_subsets, colors, line_styles):\n    # Calculate posterior parameters\n    S = np.sum(y[:n_obs])  # Number of successes\n    F = n_obs - S          # Number of failures\n    \n    alpha_post = alpha_0 + S\n    beta_post = beta_0 + F\n    \n    # Calculate posterior density\n    posterior_density = beta.pdf(theta_grid, alpha_post, beta_post)\n    \n    # Plot\n    plt.plot(theta_grid, posterior_density, color=color, linestyle=ls, linewidth=2,\n             label=f'Posterior (n={n_obs}): α={alpha_post}, β={beta_post}')\n\n# Add true theta line\nplt.axvline(true_theta, color='k', linestyle='--', \n            label=f'True θ = {true_theta}')\n\n# Customize plot\nplt.title('Evolution of Posterior Distribution for θ\\n(Bernoulli Model with Beta Prior)', fontsize=14)\nplt.xlabel('θ (probability of success)', fontsize=12)\nplt.ylabel('Probability Density', fontsize=12)\nplt.legend(fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSampling From The Posterior\nTo implement the MH algorithm assume that at iteration \\(t\\), given a sample value at iteration \\(t-1\\), \\(\\beta_{t-1}\\) we perform the following:\n\nGenerate a candidate value, \\(\\beta_c\\), from the proposal distribution \\(q(\\beta_c|\\beta_{t-1})\\).\n\nCalculate the Metropolis-Hastings ratio, MHR: \\[\n\\text{MHR}(\\beta_{t-1}, \\beta_c) = \\frac{p(\\beta_c) \\cdot q(\\beta_{t-1}|\\beta_c)}{p(\\beta_{t-1}) \\cdot q(\\beta_c|\\beta_{t-1})}\n\\] Note that this ratio can be larger than 1.\n\nGenerate a \\(\\text{Uniform}(0,1)\\) random variable \\(u\\).\n\nAccept/Reject:\n\nIf \\(u \\leq \\min(1, \\text{MHR}(\\beta_{t-1}, \\beta_c))\\), then set \\(\\beta_t = \\beta_c\\) (i.e. “keep” \\(\\beta_c\\)).\nElse, set \\(\\beta_t = \\beta_{t-1}\\).\n\n\nSet \\(t = t + 1\\) and go back to Step 1.\n\n\n# Log-posterior function (up to a constant)\ndef log_posterior(theta, y, alpha_0, beta_0):\n    if theta &lt;= 0 or theta &gt;= 1:  # Reject values outside [0,1]\n        return -np.inf\n    \n    # Log-likelihood (Bernoulli)\n    S = np.sum(y)\n    log_likelihood = S * np.log(theta) + (len(y) - S) * np.log(1 - theta)\n    \n    # Log-prior (Beta)\n    log_prior = (alpha_0 - 1) * np.log(theta) + (beta_0 - 1) * np.log(1 - theta)\n    \n    return log_likelihood + log_prior\n\n\n# Metropolis-Hastings function\ndef metropolis_hastings(y, alpha_0, beta_0, initial_theta=0.5, \n                       n_samples=10000, step_size=0.05):\n    \n    theta_samples = np.zeros(n_samples)\n    theta_current = initial_theta\n    accepted_count = 0\n    \n    for i in range(n_samples):\n        # Sample from proposal distribution (truncated normal)\n        theta_proposed = np.random.normal(theta_current, step_size)\n        \n        # Compute log acceptance ratio\n        log_alpha = (log_posterior(theta_proposed, y, alpha_0, beta_0) - \n                     log_posterior(theta_current, y, alpha_0, beta_0))\n        \n        # Accept/reject\n        if np.log(np.random.rand()) &lt; log_alpha and 0 &lt; theta_proposed &lt; 1:\n            theta_current = theta_proposed\n            accepted_count += 1\n        \n        theta_samples[i] = theta_current\n    \n    acceptance_rate = accepted_count / n_samples\n    print(f\"Acceptance rate: {acceptance_rate:.2f}\")\n    return theta_samples\n\n\n# Run MCMC\ntheta_samples = metropolis_hastings(y=y, alpha_0=alpha_0, beta_0=beta_0, \n                                  initial_theta=0.5, n_samples=15000, \n                                  step_size=0.08)\n\nAcceptance rate: 0.31\n\n\n\n# Burn-in and thinning\nburn_in = 2000\nthinned_samples = theta_samples[burn_in::5]\n\n# Plot results\nplt.figure(figsize=(12, 5))\n\n# Trace plot\nplt.subplot(1, 2, 1)\nplt.plot(thinned_samples, alpha=0.6)\nplt.xlabel(\"Iteration (thinned)\")\nplt.ylabel(\"θ\")\nplt.title(\"MCMC Trace Plot\")\nplt.axhline(true_theta, color='r', linestyle='--', label=f'True θ = {true_theta}')\nplt.legend()\n\n# Posterior histogram\nplt.subplot(1, 2, 2)\nplt.hist(thinned_samples, bins=50, density=True, alpha=0.6, \n         label='MCMC Samples')\n\n# Analytical posterior for comparison\nS = np.sum(y)\nalpha_post = alpha_0 + S\nbeta_post = beta_0 + len(y) - S\nx_grid = np.linspace(0, 1, 500)\nplt.plot(x_grid, beta.pdf(x_grid, alpha_post, beta_post), \n         'r-', label='True Posterior')\nplt.xlabel(\"θ\")\nplt.ylabel(\"Density\")\nplt.title(\"Posterior Distribution\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(f\"Posterior mean: {np.mean(thinned_samples):.3f}\")\nprint(f\"Posterior 95% credible interval: {np.percentile(thinned_samples, [2.5, 97.5])}\")\n\n\n\n\n\n\n\n\nPosterior mean: 0.308\nPosterior 95% credible interval: [0.27002716 0.34923126]"
  },
  {
    "objectID": "3_panel_data/20250417_panel_basics.html#packages",
    "href": "3_panel_data/20250417_panel_basics.html#packages",
    "title": "Panel Data Basics",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(plm)\nlibrary(broom)"
  },
  {
    "objectID": "3_panel_data/20250417_panel_basics.html#introduction",
    "href": "3_panel_data/20250417_panel_basics.html#introduction",
    "title": "Panel Data Basics",
    "section": "Introduction",
    "text": "Introduction\nWe perform a Monte Carlo simulation to demonstrate the efficiency of the random effects estimator compared to the standard OLS estimator in a panel data setting with a random effects error structure.\nThe goal is to show how the OLS estimator, which ignores the panel structure, can be inefficient when there are observation-specific unobserved effects that are uncorrelated with the covariates. In contrast, the random effects estimator exploits this structure to improve estimation precision."
  },
  {
    "objectID": "3_panel_data/20250417_panel_basics.html#model---random-effects",
    "href": "3_panel_data/20250417_panel_basics.html#model---random-effects",
    "title": "Panel Data Basics",
    "section": "Model - Random Effects",
    "text": "Model - Random Effects\nAssume we have the following panel data model:\n\\[\nY_{it} = \\alpha + X_{it} \\beta + \\eta_i + \\varepsilon_{it}\n\\]\nwhere:\n\n\\(Y_{it}\\) is the dependent variable for observation \\(i\\) at time \\(t\\)\n\n\\(X_{it}\\) is the independent variable\n\n\\(\\alpha\\) is a constant\n\n\\(\\beta\\) is the coefficient of interest\n\n\\(\\eta_i \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\\) is the observation-specific effect\n\n\\(\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)\\) is the idiosyncratic error term\n\nA key assumption here is that the observation-specific effect is uncorrelated with the covariates: \\[\n\\text{Cov}(X_{it}, \\eta_i) = 0\n\\]\nThis is the identifying assumption that justifies the use of the random effects estimator. It implies that \\(\\eta_i\\) can be treated as part of the error structure and estimated efficiently using quasi-demeaning. If this assumption were violated, random effects would be biased and inconsistent, and a fixed effects estimator would be preferred."
  },
  {
    "objectID": "3_panel_data/20250417_panel_basics.html#simulate-data",
    "href": "3_panel_data/20250417_panel_basics.html#simulate-data",
    "title": "Panel Data Basics",
    "section": "Simulate Data",
    "text": "Simulate Data\nWe simulate panel data under the assumption that the data-generating process includes a random effect. Specifically:\n\nWe vary the number of individuals \\(N\\) and hold the number of time periods \\(T\\) fixed\nThe covariate \\(X_{it}\\) is iid normal\nThe observation-specific effect \\(\\eta_{i}\\) and the idiosyncratic error term \\(\\epsilon_{it}\\) are both normal with mean 0 and variance 1\n\nThe true parameter values are:\n\n\\(\\alpha = 1\\)\n\\(\\beta = 10\\)\n\nThis setup mimics a typical balanced panel with strictly exogenous covariates and a classical random effects structure.\nThe simulation function:\n\nSimulates data for a given \\(N\\)\nEstimates both OLS and Random Effects models\nReturns coefficient estimates and confidence intervals\n\n\n# set random seed\nset.seed(1)\n\n# Parameter Values\nN_values &lt;- c(10, 25, 50, 100, 200, 500, 1000)\nT &lt;- 5\n\nalpha &lt;- 1\nbeta &lt;- 10\n\nsigma_eta &lt;- 1\nsigma_eps &lt;- 1\n\n\n# function to simulate and estimate models\nsimulate_and_estimate &lt;- function(N) {\n  \n  id &lt;- rep(1:N, each = T)\n  time &lt;- rep(1:T, times = N)\n  \n  X &lt;- rnorm(N * T)\n  eta &lt;- rep(rnorm(N, 0, sigma_eta), each = T)\n  eps &lt;- rnorm(N * T, 0, sigma_eps)\n  Y &lt;- alpha + beta * X + eta + eps\n  \n  data &lt;- tibble(id = factor(id), time, X, Y)\n  \n  ols &lt;- lm(Y ~ X, data = data)\n  re  &lt;- plm(Y ~ X, data = data, index = c(\"id\", \"time\"), model = \"random\")\n  \n  bind_rows(\n    tidy(ols, conf.int = TRUE) |&gt; mutate(model = \"OLS\", N = N),\n    tidy(re, conf.int = TRUE)  |&gt; mutate(model = \"Random Effects\", N = N)\n  )\n}\n\n\n# run simulations\nresults &lt;- map_dfr(N_values, simulate_and_estimate)"
  },
  {
    "objectID": "3_panel_data/20250417_panel_basics.html#results",
    "href": "3_panel_data/20250417_panel_basics.html#results",
    "title": "Panel Data Basics",
    "section": "Results",
    "text": "Results\nOLS ignores the panel structure entirely and treats all \\(N \\times T\\) observations as independent.\nRandom Effects correctly accounts for the within-observation correlation introduced by \\(\\eta_i\\), improving efficiency by reducing variance in the estimated coefficients.\nThe figure below plots the estimated coefficient on \\(X\\) across different values of \\(N\\). The dashed line marks the true value \\(\\beta = 10\\).\nKey points:\n\nBoth OLS and Random Effects are unbiased (on average they estimate the true coefficient)\nHowever, Random Effects is more efficient: its confidence intervals are narrower, especially at low \\(N\\)\nAs \\(N\\) increases, the precision of both estimators improves, but Random Effects consistently dominates OLS in terms of standard error\n\n\nresults %&gt;% \n  filter(term==\"X\") %&gt;% \n  ggplot(aes(x = N, y = estimate, color = model)) +\n  geom_point() +\n  geom_errorbar(\n    aes(ymin = conf.low, ymax = conf.high),\n    width = 0.1\n  ) +\n  scale_x_continuous(trans = \"log10\", breaks = N_values) +\n  geom_hline(yintercept = beta, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Coefficient Estimates with 95% Confidence Intervals\",\n    subtitle = \"Dashed line indicates true value of β = 10\",\n    y = \"Estimated Coefficient\", x = \"Number of Individuals (N)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe figure below plots the estimated coefficient on \\(\\alpha\\) across different values of \\(N\\). The dashed line marks the true value \\(\\alpha = 1\\).\nKey points:\n\nBoth OLS and Random Effects are unbiased (on average they estimate the true coefficient)\nHowever, Random Effects is less efficient: its confidence intervals are larger, especially at low \\(N\\)\nAs \\(N\\) increases, the precision of both estimators improves, but OLS consistently dominates Random Effects in terms of standard error\n\n\nresults %&gt;% \n  filter(term==\"(Intercept)\") %&gt;% \n  ggplot(aes(x = N, y = estimate, color = model)) +\n  geom_point() +\n  geom_errorbar(\n    aes(ymin = conf.low, ymax = conf.high),\n    width = 0.1\n  ) +\n  scale_x_continuous(trans = \"log10\", breaks = N_values) +\n  geom_hline(yintercept = alpha, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Coefficient Estimates with 95% Confidence Intervals\",\n    subtitle = \"Dashed line indicates true value of α = 1\",\n    y = \"Estimated Coefficient\", x = \"Number of Individuals (N)\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "3_panel_data/20250417_panel_basics.html#summary",
    "href": "3_panel_data/20250417_panel_basics.html#summary",
    "title": "Panel Data Basics",
    "section": "Summary",
    "text": "Summary\nEfficiency gains from Random Effects are largest when \\(N\\) is small and apply to the slope coefficient.\nIn real-world applications, where observation-level heterogeneity is present and uncorrelated with the covariates, Random Effects can provide more precise inference than pooled OLS."
  },
  {
    "objectID": "3_panel_data/20250425_re_vs_eb.html#packages",
    "href": "3_panel_data/20250425_re_vs_eb.html#packages",
    "title": "Random Effects vs Empirical Bayes",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(knitr)"
  },
  {
    "objectID": "3_panel_data/20250425_re_vs_eb.html#introduction",
    "href": "3_panel_data/20250425_re_vs_eb.html#introduction",
    "title": "Random Effects vs Empirical Bayes",
    "section": "Introduction",
    "text": "Introduction\nWe perform a monte carlo simulation to compare the forecasting performance of different estimators on dynamic panel data allowing for observation-level heterogeneity."
  },
  {
    "objectID": "3_panel_data/20250425_re_vs_eb.html#data-simulation",
    "href": "3_panel_data/20250425_re_vs_eb.html#data-simulation",
    "title": "Random Effects vs Empirical Bayes",
    "section": "Data Simulation",
    "text": "Data Simulation\nWe simulate panel data from an AR(1) with an intercept. The data generating process is taken from Section 6.1 of Pesaran et al (2024) and is is defined as:\n\\[\ny_{it} = \\alpha_i + \\beta_i y_{i,t-1} + \\varepsilon_{it}\n\\] for each observation \\(i\\) and each time period \\(t\\).\nWe allow for the possibility of different intercepts for each observation:\n\\[\n\\alpha_i \\sim \\mathcal{N}(\\alpha_{0i}, \\sigma^2_{\\alpha})\n\\]\nWe allow for the possibility of different slopes for each observation:\n\\[\n\\beta_i = \\beta_0 + \\eta_{i\\beta} \\qquad \\eta_{i\\beta} \\sim U(-a_{\\beta}/2, a_{\\beta}/2)\n\\]\nWe allow for observation-specific heteroskedasticity:\n\\[\n\\varepsilon_{it} = \\sigma_i \\cdot \\frac{z_{it}^2 - 1}{\\sqrt{2}} \\qquad z_{it} \\sim \\mathcal{N}(0, 1) \\qquad \\sigma_i^2 \\sim 0.5 + 0.5 \\cdot \\chi^2_1\n\\]\nThe initial value for each observation is drawn from the stationary distribution of the process:\n\\[\ny_{i1} \\sim \\mathcal{N}\\left( \\frac{\\alpha}{1 - \\beta}, \\frac{\\sigma^2}{1 - \\beta^2} \\right)\n\\]\n\nsimulate_panel_data &lt;- function(\n    N = 50, T = 20, sigma2_alpha = 0, a_beta = 0, alpha = 1, beta = 0.8\n  ) {\n\n  sim_data &lt;- tibble()\n\n  for (i in 1:N) {\n\n    # draw observation-specific beta_i and alpha_i\n    beta_i &lt;- beta + runif(1, min = -a_beta / 2, max = a_beta / 2)\n    alpha_i &lt;- alpha + rnorm(1, mean = 0, sd = sqrt(sigma2_alpha))\n\n    # draw sigma_i^2 from 0.5 + 0.5 * chi-squared(1)\n    sigma2_i &lt;- 0.5 + 0.5 * rchisq(1, df = 1)\n    sigma_i &lt;- sqrt(sigma2_i)\n\n    # simulate data for T+1 periods\n    y &lt;- numeric(T + 1)\n\n    # start at stationary distribution\n    y[1] &lt;- rnorm(1, mean=alpha_i/(1-beta_i), sd=sigma_i/sqrt(1-beta_i^2))\n\n    # simulate remaining values\n    for (t in 2:(T + 1)) {\n\n      # draw standard normal z_it\n      z_it &lt;- rnorm(1, mean = 0, sd = 1)\n\n      # error term\n      epsilon_it &lt;- sigma_i * (z_it^2 - 1) / sqrt(2)\n\n      # next value\n      y[t] &lt;- alpha_i + beta_i * y[t - 1] + epsilon_it\n\n    }\n\n    # combine data together into tibble\n    df &lt;- tibble(\n      id = i,\n      alpha=alpha_i,\n      beta=beta_i,\n      sigma=sigma_i,\n      t = 1:(T + 1),\n      y = y,\n      lag_y = dplyr::lag(y)\n    )\n\n    sim_data &lt;- bind_rows(sim_data, df)\n  }\n\n  list(\n    estimation_data = sim_data %&gt;% filter(t &lt;= T),\n    forecast_data = sim_data %&gt;% filter(t == T + 1)\n  )\n}"
  },
  {
    "objectID": "3_panel_data/20250425_re_vs_eb.html#model-estimation",
    "href": "3_panel_data/20250425_re_vs_eb.html#model-estimation",
    "title": "Random Effects vs Empirical Bayes",
    "section": "Model Estimation",
    "text": "Model Estimation\nWe simulate \\(T + 1\\) periods for each observation \\(i = 1, ..., N\\). For each simulated panel:\n\nThe first \\(T\\) periods are used for model estimation\nThe final period \\(t = T+1\\) is used for one-step-ahead forecast evaluation\n\nWe estimate four models, each differing in how it handles observation-level heterogeneity in the data. The goal is to evaluate how different assumptions about heterogeneity and information pooling affect forecast accuracy. The models are:\n\n1. Observation-Specific OLS\nA separate ordinary least squares (OLS) regression is estimated for each observation \\(i\\), using only that observation’s time series data:\n\\[\ny_{it} = \\alpha_i + \\beta_i y_{i,t-1} + \\varepsilon_{it}\n\\]\n\nPros: Fully flexible—each observation has its own intercept and slope.\n\nCons: Very noisy, especially with short time series (small \\(T\\)); no pooling across observations.\n\nRole: Used as the baseline for comparing forecast performance.\n\n\n\n2. Pooled OLS\nA single regression is estimated across all observations, assuming homogeneous parameters:\n\\[\ny_{it} = \\alpha + \\beta y_{i,t-1} + \\varepsilon_{it}\n\\]\n\nPros: Very efficient; low variance due to maximum pooling.\n\nCons: Ignores heterogeneity; biased if observation effects are substantial.\n\n\n\n3. Random Effects (RE) Model\nWe estimate a random coefficients model using mixed-effects regression:\n\\[\ny_{it} = (\\alpha + \\eta_i) + (\\beta + \\zeta_i) y_{i,t-1} + \\varepsilon_{it}\n\\]\nwith\n\\[\n\\begin{bmatrix} \\eta_i \\\\ \\zeta_i \\end{bmatrix} \\sim \\mathcal{N}(0, \\Omega), \\quad \\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\n\nPros: Balances bias and variance trade-off by partially pooling toward a common mean.\n\nCons: Relies on distributional assumptions (e.g., normality of random effects); estimation can be complex with small samples.\n\n\n\n4. Empirical Bayes (EB)\nThis model explicitly applies Empirical Bayes shrinkage to individual-specific OLS estimates. It assumes:\n\nThe individual-level parameters \\(\\theta_i = (\\alpha_i, \\beta_i)'\\) follow a multivariate normal prior:\n\n\\[\n\\theta_i \\sim \\mathcal{N}(\\bar{\\theta}, \\Omega)\n\\]\n\nThe posterior mean (BLUP) is used as the shrinkage estimator:\n\n\\[\n\\hat{\\theta}_i^{EB} = \\left(W_i'W_i / \\sigma_i^2 + \\Omega^{-1} \\right)^{-1} \\left(W_i'y_i / \\sigma_i^2 + \\Omega^{-1} \\bar{\\theta} \\right)\n\\]\n\nPros: Data-adaptive shrinkage toward a cross-sectional prior; flexible and often robust to overfitting.\n\nCons: Requires variance estimates and assumptions about the distribution of heterogeneity.\n\nTogether, these models form a spectrum of pooling strategies:\n\n\n\n\n\n\n\n\n\nModel\nDegree of Pooling\nHandles Heterogeneity?\nRisk of Overfitting\n\n\n\n\nIndividual OLS\nNone\nFully\nHigh (esp. with low \\(T\\))\n\n\nPooled OLS\nFull\nIgnores\nLow\n\n\nRandom Effects\nPartial\nYes (via random effects)\nMedium\n\n\nEmpirical Bayes\nData-driven shrinkage\nYes (via shrinkage)\nMedium\n\n\n\nThis framework allows us to test the forecast trade-off between flexibility (observation-specific fit) and efficiency (pooling), with forecast accuracy at \\(T + 1\\) serving as the performance metric."
  },
  {
    "objectID": "3_panel_data/20250425_re_vs_eb.html#forecast-evaluation",
    "href": "3_panel_data/20250425_re_vs_eb.html#forecast-evaluation",
    "title": "Random Effects vs Empirical Bayes",
    "section": "Forecast Evaluation",
    "text": "Forecast Evaluation\nForecasts are computed for period \\(T+1\\), and mean squared forecast errors (MSFEs) are calculated by comparing predicted values to the actual simulated outcome.\nLet \\(\\hat{y}_{i,T+1}^{(m)}\\) denote the forecast for observation \\(i\\) using method \\(m \\in \\{\\text{observation}, \\text{pooled}, \\text{re}, \\text{be}\\}\\). Then the MSFE for method \\(m\\) is:\n\\[\n\\text{MSFE}^{(m)} = \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{y}_{i,T+1}^{(m)} - y_{i,T+1} \\right)^2\n\\]\nTo assess the relative performance of the panel methods, we compute relative MSFEs by dividing each method’s MSFE by that of the observation-specific forecasts:\n\\[\n\\text{Relative MSFE}^{(m)} = \\frac{\\text{MSFE}^{(m)}}{\\text{MSFE}^{(\\text{observation})}}, \\quad m \\in \\{\\text{pooled}, \\text{re}, \\text{be} \\}\n\\]\nThis benchmarking helps identify how much more or less accurate the panel estimators are relative to the baseline model.\n\nrun_simulation &lt;- function(\n    N = 50, T = 20, sigma2_alpha = 0, a_beta = 0, alpha = 1, beta = 0.8\n  ) {\n\n  # simulate data for chosen values of N and T\n  data_list &lt;- simulate_panel_data(\n    N = N, T = T, sigma2_alpha = sigma2_alpha, a_beta = a_beta, \n    alpha = alpha, beta = beta\n  )\n\n  # retreieve estimation data\n  estimation_data &lt;- data_list$estimation_data %&gt;% \n    drop_na(lag_y) %&gt;%\n    mutate(id = factor(id))\n\n  # retrieve forecast data\n  forecast_data &lt;- data_list$forecast_data %&gt;% \n    mutate(id = factor(id, levels = levels(estimation_data$id)))\n\n  \n  ### OBSERVATION-SPECIFIC MODELS ###\n  forecast_data$yhat_individual &lt;- NA\n  \n  # list to store estimated intercept and slope coefficient\n  theta_individual &lt;- list()\n  \n  # vector to store estimated error variance\n  sigma2_individual &lt;- numeric(0)\n\n  for (i in unique(forecast_data$id)) {\n    \n    # data relevant to this observation\n    est_i &lt;- estimation_data %&gt;% \n      filter(id == i)\n    \n    # check sufficient data\n    if (nrow(est_i) &gt;= 2) {\n      \n      # estimate individual specific ols\n      model_i &lt;- lm(y ~ lag_y, data = est_i)\n      \n      # store coefficients\n      theta_individual[[i]] &lt;- coef(model_i)\n      \n      # store estimated variance\n      sigma2_individual[i] &lt;- mean(resid(model_i)^2)\n      \n      # make one step ahead forecast\n      forecast_data$yhat_individual[forecast_data$id == i] &lt;- \n        predict(model_i, newdata = forecast_data %&gt;% filter(id == i))\n      \n    } else {\n      \n      beta_individual[[i]] &lt;- c(NA, NA)\n      sigma2_individual[i] &lt;- NA\n      \n    }\n  }\n\n  \n  ### POOLING MODEL ###\n  pooled_model &lt;- lm(y ~ lag_y, data = estimation_data)\n  forecast_data$yhat_pooled &lt;- predict(pooled_model, newdata = forecast_data)\n  \n  # Extract PR² from pooled model\n  pr2 &lt;- summary(pooled_model)$r.squared\n  \n  \n  ### RANDOM EFFECTS MODEL ###\n  re_model &lt;- lmer(y ~ lag_y + (lag_y | id), data=estimation_data, REML=TRUE)\n  forecast_data$yhat_re &lt;- predict(re_model, newdata = forecast_data)\n  \n  \n  ### EMPIRICAL BAYES MODEL ###\n  \n  # matrix of estimated coefficients\n  theta_mat &lt;- do.call(rbind, theta_individual)\n  \n  # ids for which we have estimates\n  valid_ids &lt;- which(!is.na(theta_mat[,1]))\n  \n  # mean of intercept and slope coefficients\n  theta_bar &lt;- colMeans(theta_mat[valid_ids, , drop = FALSE])\n  \n  # estimated covariance matrix\n  Omega_hat &lt;- cov(theta_mat[valid_ids, , drop = FALSE])\n\n  forecast_data$yhat_eb &lt;- NA\n  \n  for (i in valid_ids) {\n\n    # matrix of predictors\n    W_i &lt;- model.matrix(y ~ lag_y, data = estimation_data %&gt;% filter(id == i))\n\n    # matrix of dependenet variables\n    y_i &lt;- estimation_data %&gt;% filter(id == i) %&gt;% pull(y)\n\n    # estimated variance for observation i\n    sigma2_i &lt;- sigma2_individual[i]\n\n    # naive bayes estimator\n    A &lt;- solve(t(W_i) %*% W_i / sigma2_i + solve(Omega_hat))\n    b &lt;- t(W_i) %*% y_i / sigma2_i + solve(Omega_hat) %*% theta_bar\n    theta_eb &lt;- A %*% b\n\n    # regressors for individual i\n    x_T1 &lt;- model.matrix(~ lag_y, data = forecast_data %&gt;% filter(id == i))\n    \n    # make forecast\n    forecast_data$yhat_eb[forecast_data$id == i] &lt;- as.numeric(x_T1 %*% theta_eb)\n    \n  }\n  \n\n  # Compute RMSFEs\n  results &lt;- forecast_data %&gt;%\n    summarise( # Compute MSFEs\n      individual = mean((yhat_individual - y)^2, na.rm = TRUE),\n      pooled = mean((yhat_pooled - y)^2, na.rm = TRUE),\n      re = mean((yhat_re - y)^2, na.rm = TRUE),\n      eb = mean((yhat_eb - y)^2, na.rm = TRUE)\n    ) %&gt;%\n    mutate( # Compute RELATIVE MSFEs\n      pooled = pooled / individual,\n      re = re / individual,\n      eb = eb / individual\n    ) %&gt;%\n    # drop individual, because we are expressing relative to it\n    select(pooled, re, eb) %&gt;%\n    mutate(pr2 = pr2) # add pr2 from pooled regressions\n\n}"
  },
  {
    "objectID": "3_panel_data/20250425_re_vs_eb.html#simulation",
    "href": "3_panel_data/20250425_re_vs_eb.html#simulation",
    "title": "Random Effects vs Empirical Bayes",
    "section": "Simulation",
    "text": "Simulation\nWe conduct a grid simulation to evaluate the forecast performance of each model under a range of data-generating processes (DGPs). The simulation explores how estimator performance varies with:\n\n\\(T\\): The number of time periods per observation\n\\(\\sigma^2_\\alpha\\): The variance of observation-level intercept heterogeneity\n\\(a_\\beta\\): The degree of heterogeneity in observation-specific slopes\n\n\nrun_grid_simulation &lt;- function(\n    B, N_vals, T_vals, sigma2_alpha_vals, a_beta_vals, alpha, beta\n  ) {\n  \n  grid &lt;- expand_grid(\n    N = N_vals, T = T_vals, sigma2_alpha = sigma2_alpha_vals, \n    a_beta = a_beta_vals\n  )\n  \n  results_all &lt;- grid %&gt;%\n    mutate(\n      result = pmap(list(N, T, a_beta, sigma2_alpha), function(Ni, Ti, ab, sa) {\n        message(sprintf(\"Running simulation for N = %d, T = %d, a_beta = %.2f, \n                        sigma2_alpha = %.2f\", Ni, Ti, ab, sa))\n        sims &lt;- map_dfr(1:B, ~run_simulation(\n          N = Ni, T = Ti, sigma2_alpha = sa, a_beta = ab, alpha = alpha, \n          beta = beta))\n        sims %&gt;% summarise(across(everything(), mean))\n      })\n    ) %&gt;%\n    unnest(result)\n\n  return(results_all)\n  \n}\n\n\n# set random seed\nset.seed(1)\n\n# parameters\nB &lt;- 5\nN_vals &lt;- c(100)\nT_vals &lt;- c(20, 50)\nsigma2_alpha_vals &lt;- c(0, 0.5, 1)\na_beta_vals &lt;- c(0, 0.25, 0.5)\nalpha &lt;- 1\nbeta &lt;- 0.5\n\n\ngrid_results &lt;- run_grid_simulation(\n  B = B,\n  N_vals = N_vals,\n  T_vals = T_vals,\n  sigma2_alpha_vals = sigma2_alpha_vals,\n  a_beta_vals = a_beta_vals,\n  alpha = alpha,\n  beta = beta\n)"
  },
  {
    "objectID": "3_panel_data/20250425_re_vs_eb.html#results-interpretation",
    "href": "3_panel_data/20250425_re_vs_eb.html#results-interpretation",
    "title": "Random Effects vs Empirical Bayes",
    "section": "Results Interpretation",
    "text": "Results Interpretation\nThe simulation results reveal several clear patterns regarding the relative forecast performance of the different estimators.\n\nHomogeneous Case: No Heterogeneity (\\(\\sigma^2_\\alpha = 0\\), \\(a_\\beta = 0\\))\nWhen the data-generating process is fully homogeneous—i.e., all observations share the same intercept and slope—the pooled OLS model should deliver the best forecast performance as pooling maximizes statistical efficiency when the true model is the same across observations.\nIn this setting, all three panel estimators—pooled, random effects (RE), and empirical Bayes (EB)—consistently outperform the observation-specific OLS model. The observation-specific model overfits due to the small sample size per observation and fails to take advantage of shared structure across observations.\n\n# homogeneous case\nresults &lt;- grid_results %&gt;%\n  filter(sigma2_alpha==0 & a_beta==0) %&gt;% \n  mutate(across(c(pooled, re, eb, pr2), ~ round(., 3)))\n\n\nkableExtra::kbl(\n  results,\n  col.names = c(\n    'N', \n    'T', \n    'σ²&lt;sub&gt;α&lt;/sub&gt;',\n    'a&lt;sub&gt;β&lt;/sub&gt;',\n    'Pooled', \n    'RE', \n    'EB', \n    'PR²'\n  ),\n  caption = 'Homogeneous Case',\n  escape = FALSE\n  ) %&gt;%\n  kableExtra::kable_styling()\n\n\nHomogeneous Case\n\n\nN\nT\nσ²α\naβ\nPooled\nRE\nEB\nPR²\n\n\n\n\n100\n20\n0\n0\n0.793\n0.791\n0.831\n0.232\n\n\n100\n50\n0\n0\n0.947\n0.948\n0.953\n0.254\n\n\n\n\n\n\n\nMedium Heterogeneity (\\(\\sigma^2_\\alpha &gt; 0\\), \\(a_\\beta &gt; 0\\))\nWhen moderate heterogeneity is introduced into both intercepts and slopes, the performance of the pooled model deteriorates, as it no longer correctly reflects the data structure. However, the RE and EB estimators adapt to the heterogeneity through partial pooling and outperform the observation-specific model.\nThis reflects the value of shrinkage: RE and EB methods reduce forecast variance without introducing substantial bias, especially when \\(T\\) is small.\n\n# medium heterogeneity\nresults &lt;-grid_results %&gt;%\n  filter(sigma2_alpha==0.5 & a_beta==0.25) %&gt;% \n  mutate(across(c(pooled, re, eb, pr2), ~ round(., 3)))\n\n\nkableExtra::kbl(\n  results,\n  col.names = c(\n    'N', \n    'T', \n    'σ²&lt;sub&gt;α&lt;/sub&gt;',\n    'a&lt;sub&gt;β&lt;/sub&gt;',\n    'Pooled', \n    'RE', \n    'EB', \n    'PR²'\n  ),\n  caption = 'Medium Heterogeneity',\n  escape = FALSE\n  ) %&gt;% \n  kableExtra::kable_styling()\n\n\nMedium Heterogeneity\n\n\nN\nT\nσ²α\naβ\nPooled\nRE\nEB\nPR²\n\n\n\n\n100\n20\n0.5\n0.25\n1.161\n0.934\n0.934\n0.687\n\n\n100\n50\n0.5\n0.25\n1.166\n0.999\n0.999\n0.695\n\n\n\n\n\n\n\nHigh Heterogeneity (\\(\\sigma^2_\\alpha = 1\\), \\(a_\\beta = 0.5\\))\nEven in the presence of substantial heterogeneity, the RE and EB models maintain a forecast advantage over the observation-specific model. While the benefit from pooling is reduced compared to the homogeneous case, the shrinkage estimators still provide a better bias–variance tradeoff, particularly in short panels.\nNotably, the pooled model performs poorly in this case, as it imposes a strong (and incorrect) homogeneity assumption.\n\n# high heterogeneity\n results &lt;- grid_results %&gt;%\n  filter(sigma2_alpha==1 & a_beta==0.5) %&gt;% \n  mutate(across(c(pooled, re, eb, pr2), ~ round(., 3)))\n\n\nkableExtra::kbl(\n  results,\n  col.names = c(\n    'N', \n    'T', \n    'σ²&lt;sub&gt;α&lt;/sub&gt;',\n    'a&lt;sub&gt;β&lt;/sub&gt;',\n    'Pooled', \n    'RE', \n    'EB', \n    'PR²'\n  ),\n  caption = 'High Heterogeneity',\n  escape = FALSE\n  ) %&gt;% \n  kableExtra::kable_styling()\n\n\nHigh Heterogeneity\n\n\nN\nT\nσ²α\naβ\nPooled\nRE\nEB\nPR²\n\n\n\n\n100\n20\n1\n0.5\n1.322\n0.984\n0.958\n0.822\n\n\n100\n50\n1\n0.5\n1.317\n0.996\n0.989\n0.806\n\n\n\n\n\n\n\nRole of Time Series Length (\\(T\\))\nAcross all levels of heterogeneity, the relative advantage of RE and EB models is most pronounced when \\(T = 20\\). With limited time series data, the observation-specific OLS estimates are noisy and overfit the training data. In this setting, partial pooling provides substantial forecast gains.\nAs \\(T\\) increases to \\(T = 50\\), the observation-specific estimates become more stable, and the benefits of shrinkage diminish. This is intuitive: with longer time series, each observation’s model can be estimated more reliably, reducing the need to borrow strength from others."
  },
  {
    "objectID": "3_panel_data/20250425_re_vs_eb.html#summary",
    "href": "3_panel_data/20250425_re_vs_eb.html#summary",
    "title": "Random Effects vs Empirical Bayes",
    "section": "Summary",
    "text": "Summary\nKey results:\n\nPooling is optimal under homogeneity, with pooled OLS performing best\nRE and EB estimators provide robust performance across a range of heterogeneous DGPs, outperforming observation-sepcific OLS in all scenarios considered\nShrinkage methods are most beneficial in short panels (small \\(T\\)), where observation-sepcific models are noisy\nAs \\(T\\) grows, the gap between methods narrows, validating the consistency of observation-specific estimators in long panels\n\nThese results highlight the practical importance of partial pooling methods like RE and EB in empirical applications where panel length is short and individual heterogeneity is likely.\n\n# all results\nresults &lt;- grid_results %&gt;% \n  mutate(across(c(pooled, re, eb, pr2), ~ round(., 3)))\n\n\nkableExtra::kbl(\n  results,\n  col.names = c(\n    'N', \n    'T', \n    'σ²&lt;sub&gt;α&lt;/sub&gt;',\n    'a&lt;sub&gt;β&lt;/sub&gt;',\n    'Pooled', \n    'RE', \n    'EB', \n    'PR²'\n  ),\n  caption = 'All Cases',\n  escape = FALSE\n  ) %&gt;% \n  kableExtra::kable_styling()\n\n\nAll Cases\n\n\nN\nT\nσ²α\naβ\nPooled\nRE\nEB\nPR²\n\n\n\n\n100\n20\n0.0\n0.00\n0.793\n0.791\n0.831\n0.232\n\n\n100\n20\n0.0\n0.25\n0.922\n0.915\n0.945\n0.285\n\n\n100\n20\n0.0\n0.50\n0.926\n0.897\n0.923\n0.442\n\n\n100\n20\n0.5\n0.00\n1.000\n0.862\n0.877\n0.613\n\n\n100\n20\n0.5\n0.25\n1.161\n0.934\n0.934\n0.687\n\n\n100\n20\n0.5\n0.50\n1.171\n0.993\n0.986\n0.738\n\n\n100\n20\n1.0\n0.00\n1.248\n0.883\n0.870\n0.754\n\n\n100\n20\n1.0\n0.25\n0.966\n0.887\n0.897\n0.784\n\n\n100\n20\n1.0\n0.50\n1.322\n0.984\n0.958\n0.822\n\n\n100\n50\n0.0\n0.00\n0.947\n0.948\n0.953\n0.254\n\n\n100\n50\n0.0\n0.25\n0.975\n0.964\n0.981\n0.289\n\n\n100\n50\n0.0\n0.50\n1.065\n0.972\n0.975\n0.425\n\n\n100\n50\n0.5\n0.00\n1.210\n0.953\n0.961\n0.643\n\n\n100\n50\n0.5\n0.25\n1.166\n0.999\n0.999\n0.695\n\n\n100\n50\n0.5\n0.50\n1.212\n0.980\n0.977\n0.732\n\n\n100\n50\n1.0\n0.00\n1.266\n0.970\n0.974\n0.789\n\n\n100\n50\n1.0\n0.25\n1.377\n0.979\n0.977\n0.795\n\n\n100\n50\n1.0\n0.50\n1.317\n0.996\n0.989\n0.806"
  },
  {
    "objectID": "5_neural_networks/20250611_basic_nn.html",
    "href": "5_neural_networks/20250611_basic_nn.html",
    "title": "Simple Feedforward Neural Network",
    "section": "",
    "text": "Simple Feedforward Neural Network\n\n\nPackages\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport graphviz\nfrom graphviz import Digraph\nfrom IPython.display import display, Markdown\n\n\n\nIntroduction\nIn this notebook, we explore one full forward pass of a simple feedforward neural network. The network architecture consists of:\n\n2 input neurons\n1 hidden layer with 3 neurons and ReLU activation\n2 output neurons (no activation, suitable for regression)\n\nWe manually specify the weights, biases, and learning rate.\n\n\nNeural Network Setup\nWe consider a simple feedforward neural network with 2 input neurons, 1 hidden layer with 3 neurons, and 2 output neurons. The following describes its structure:\n\nInput vector:\n\\[\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\in \\mathbb{R}^2\\]\nWeight matrices and bias vectors:\nHidden layer weights: \\[\nW^{(1)} =\n\\begin{bmatrix}\nW^{(1)}_{11} & W^{(1)}_{12} \\\\\nW^{(1)}_{21} & W^{(1)}_{22} \\\\\nW^{(1)}_{31} & W^{(1)}_{32}\n\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}, \\quad\n\\mathbf{b}^{(1)} =\n\\begin{bmatrix}\nb^{(1)}_1 \\\\\nb^{(1)}_2 \\\\\nb^{(1)}_3\n\\end{bmatrix} \\in \\mathbb{R}^3\n\\]\nOutput layer weights: \\[\nW^{(2)} =\n\\begin{bmatrix}\nW^{(2)}_{11} & W^{(2)}_{12} & W^{(2)}_{13} \\\\\nW^{(2)}_{21} & W^{(2)}_{22} & W^{(2)}_{23}\n\\end{bmatrix} \\in \\mathbb{R}^{2 \\times 3}, \\quad\n\\mathbf{b}^{(2)} =\n\\begin{bmatrix}\nb^{(2)}_1 \\\\\nb^{(2)}_2\n\\end{bmatrix} \\in \\mathbb{R}^2\n\\]\n\n\nForward Pass\n\nHidden layer (before activation):\n\\[\\mathbf{z}^{(1)} = W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}\\]\nHidden layer activation (ReLU):\n\\[\\mathbf{h} = \\phi(\\mathbf{z}^{(1)}) \\quad \\text{(applied elementwise)}\\]\nOutput layer:\n\\[\\mathbf{y} = W^{(2)} \\mathbf{h} + \\mathbf{b}^{(2)}\\]\n\n\n\nCombined Expression\nThe full network can be written as: \\[\n\\mathbf{y} = W^{(2)} \\cdot \\phi(W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}) + \\mathbf{b}^{(2)}\n\\]\n\n\nTarget Function\nThe target output vector is defined as: \\[\n\\mathbf{t} = f(x_1, x_2) = \\begin{bmatrix}\nx_1^2 + x_2^2 \\\\\nx_1 \\cdot x_2\n\\end{bmatrix}\n\\]\n\n\nLoss Function\nWe use the mean squared error (MSE) loss (with factor $ \\():\\)$ = | - |^2 = _{i=1}^2 (y_i - t_i)^2 $$\n\n\nDiagram\nThe network can be represented diagramatically by the following:\n\ndef draw_neural_net():\n    dot = Digraph(format='png')\n    dot.attr(rankdir='LR', nodesep='1.0')\n\n    # Input layer\n    input_nodes = ['x1', 'x2']\n    for node in input_nodes:\n        dot.node(node, shape='circle', style='filled', fillcolor='lightgray')\n\n    # Hidden layer\n    hidden_nodes = ['h1', 'h2', 'h3']\n    for node in hidden_nodes:\n        dot.node(node, shape='circle', style='filled', fillcolor='lightblue')\n\n    # Output layer\n    output_nodes = ['y1', 'y2']\n    for node in output_nodes:\n        dot.node(node, shape='circle', style='filled', fillcolor='lightgreen')\n\n    # Add a subgraph for each layer to group and rank nodes\n    with dot.subgraph() as s:\n        s.attr(rank='same')\n        s.node('input_label', label='Input Layer', shape='plaintext')\n        s.edge('input_label', 'x1', style='invis')\n        s.edge('input_label', 'x2', style='invis')\n\n    with dot.subgraph() as s:\n        s.attr(rank='same')\n        s.node('hidden_label', label='Hidden Layer', shape='plaintext')\n        s.edge('hidden_label', 'h1', style='invis')\n        s.edge('hidden_label', 'h2', style='invis')\n        s.edge('hidden_label', 'h3', style='invis')\n\n    with dot.subgraph() as s:\n        s.attr(rank='same')\n        s.node('output_label', label='Output Layer', shape='plaintext')\n        s.edge('output_label', 'y1', style='invis')\n        s.edge('output_label', 'y2', style='invis')\n\n    # Connect input to hidden\n    for i_node in input_nodes:\n        for h_node in hidden_nodes:\n            dot.edge(i_node, h_node)\n\n    # Connect hidden to output\n    for h_node in hidden_nodes:\n        for o_node in output_nodes:\n            dot.edge(h_node, o_node)\n\n    return dot\n\n# Display the diagram inline\ndisplay(draw_neural_net())\n\n\n\n\n\n\n\n\n\n\n\nRunning the Neural Network For One Iteration Using Pytorch\nWe initialize the neural network with the following values:\nThe input vector \\(\\mathbf{x} \\in \\mathbb{R}^2\\) is:\n\\[\n\\mathbf{x} =\n\\begin{bmatrix}\n1.0 \\\\\n2.0\n\\end{bmatrix}\n\\]\nThe target vector \\(\\mathbf{t} \\in \\mathbb{R}^2\\) is:\n\\[\n\\mathbf{t} =\n\\begin{bmatrix}\n5.0 \\\\\n2.0\n\\end{bmatrix}\n\\]\nWeight matrix \\(W^{(1)} \\in \\mathbb{R}^{3 \\times 2}\\):\n\\[\nW^{(1)} =\n\\begin{bmatrix}\n0.1 & -0.2 \\\\\n0.4 & \\phantom{-}0.1 \\\\\n-0.3 & \\phantom{-}0.2\n\\end{bmatrix}\n\\]\nBias vector \\(\\mathbf{b}^{(1)} \\in \\mathbb{R}^3\\):\n\\[\n\\mathbf{b}^{(1)} =\n\\begin{bmatrix}\n0.0 \\\\\n0.0 \\\\\n0.0\n\\end{bmatrix}\n\\]\nWeight matrix \\(W^{(2)} \\in \\mathbb{R}^{2 \\times 3}\\):\n\\[\nW^{(2)} =\n\\begin{bmatrix}\n0.2 & -0.1 & \\phantom{-}0.3 \\\\\n-0.3 & \\phantom{-}0.5 & \\phantom{-}0.1\n\\end{bmatrix}\n\\]\nBias vector \\(\\mathbf{b}^{(2)} \\in \\mathbb{R}^2\\):\n\\[\n\\mathbf{b}^{(2)} =\n\\begin{bmatrix}\n0.0 \\\\\n0.0\n\\end{bmatrix}\n\\]\n\n# Define the updated neural network class for the (2 -&gt; 3 -&gt; 2) architecture\nclass TwoLayerNN(nn.Module):\n    def __init__(self):\n        super(TwoLayerNN, self).__init__()\n        # Manually define parameters so we can initialize them explicitly\n        self.W1 = nn.Parameter(torch.tensor([[0.1, -0.2],\n                                             [0.4, 0.1],\n                                             [-0.3, 0.2]], dtype=torch.float32))  # (3, 2)\n        self.b1 = nn.Parameter(torch.zeros(3, dtype=torch.float32))  # (3,)\n        self.W2 = nn.Parameter(torch.tensor([[0.2, -0.1, 0.3],\n                                             [-0.3, 0.5, 0.1]], dtype=torch.float32))  # (2, 3)\n        self.b2 = nn.Parameter(torch.zeros(2, dtype=torch.float32))  # (2,)\n\n    def forward(self, x):\n        z1 = self.W1 @ x + self.b1.unsqueeze(1)       # (3,1)\n        h = torch.relu(z1)                            # (3,1)\n        z2 = self.W2 @ h + self.b2.unsqueeze(1)       # (2,1)\n        return z2, h, z1  # output, hidden activation, hidden pre-activation\n\n\n# Initialize model\nmodel = TwoLayerNN()\n\n\n# Input and target\nx = torch.tensor([[1.0], [2.0]])  # shape (2,1)\nt = torch.tensor([[5.0], [2.0]])  # target output\n\nWe use a learning rate of:\n\\[\n\\eta = 0.01\n\\]\n\n# Learning rate\neta = 0.01\n\nWe run the neural network using the innitial weights and input vector and calculate the loss:\n\n# Forward pass\ny, h, z1 = model(x)\nloss = 0.5 * torch.sum((y - t) ** 2)\n\nWe then compute the backpropagation equations:\nBackpropagate to Output Layer\n\\[\n\\delta^{(2)} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} = \\mathbf{y} - \\mathbf{t}\n\\]\nGradients for Output Layer Parameters\nWeights:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}} =\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\right) \\cdot \\left( \\frac{\\partial \\mathbf{y}}{\\partial W^{(2)}} \\right) =\n(\\mathbf{y} - \\mathbf{t}) \\cdot \\mathbf{h}^\\top\n\\]\nBiases:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(2)}} =\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\right) \\cdot \\left( \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{b}^{(2)}} \\right) =\n\\mathbf{y} - \\mathbf{t}\n\\]\nBackpropagate to Hidden Layer\nGradient w.r.t. hidden activations:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} =\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\right) \\cdot \\left( \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{h}} \\right)\n= (W^{(2)})^\\top (\\mathbf{y} - \\mathbf{t})\n\\]\nDefine hidden layer error:\n\\[\n\\delta^{(1)} =\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(1)}} \\right) =\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} \\right) \\odot \\left( \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}^{(1)}} \\right) =\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} \\right) \\odot \\phi'(\\mathbf{z}^{(1)}) =\n(W^{(2)})^\\top (\\mathbf{y} - \\mathbf{t}) \\odot \\phi'(\\mathbf{z}^{(1)})\n\\]\nGradients for Hidden Layer Parameters\nWeights:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} =\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(1)}} \\right) \\cdot \\left( \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial W^{(1)}} \\right) =\n\\delta^{(1)} \\cdot \\mathbf{x}^\\top\n\\]\nBiases:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(1)}} =\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(1)}} \\right) =\n\\delta^{(1)}\n\\]\n\n# Backward pass\nloss.backward()\n\nAfter computing the gradients of the loss function with respect to all parameters, we update the weights and biases using gradient descent:\nOutput layer weights and biases:\n\\[\nW^{(2)} \\leftarrow W^{(2)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{(2)}}\n\\]\n\\[\n\\mathbf{b}^{(2)} \\leftarrow \\mathbf{b}^{(2)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(2)}}\n\\]\nHidden layer weights and biases:\n\\[\nW^{(1)} \\leftarrow W^{(1)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}}\n\\]\n\\[\n\\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(1)}}\n\\]\n\n# Manually update weights and biases using gradients\nwith torch.no_grad():\n    model.W1 -= eta * model.W1.grad\n    model.b1 -= eta * model.b1.grad\n    model.W2 -= eta * model.W2.grad\n    model.b2 -= eta * model.b2.grad\n\n\n# Prepare to display updated weights and biases\nupdated_params = {\n    \"W1\": model.W1.detach().numpy(),\n    \"b1\": model.b1.detach().numpy(),\n    \"W2\": model.W2.detach().numpy(),\n    \"b2\": model.b2.detach().numpy(),\n    \"output y\": y.detach().numpy().flatten(),\n    \"loss\": loss.item()\n}\n\n\nW1_df = pd.DataFrame(updated_params[\"W1\"], columns=[\"x1\", \"x2\"], index=[\"h1\", \"h2\", \"h3\"])\nW2_df = pd.DataFrame(updated_params[\"W2\"], columns=[\"h1\", \"h2\", \"h3\"], index=[\"y1\", \"y2\"])\nb1_df = pd.DataFrame(updated_params[\"b1\"], index=[\"h1\", \"h2\", \"h3\"], columns=[\"b1\"])\nb2_df = pd.DataFrame(updated_params[\"b2\"], index=[\"y1\", \"y2\"], columns=[\"b2\"])\n\nThe updated weights and biases are:\n\n# Display all updated parameters nicely\ndisplay(Markdown(\"### Updated Parameters\"))\n\ndisplay(Markdown(\"**Hidden Layer Weights (W1):**\"))\ndisplay(W1_df)\n\ndisplay(Markdown(\"**Hidden Layer Biases (b1):**\"))\ndisplay(b1_df)\n\ndisplay(Markdown(\"**Output Layer Weights (W2):**\"))\ndisplay(W2_df)\n\ndisplay(Markdown(\"**Output Layer Biases (b2):**\"))\ndisplay(b2_df)\n\nUpdated Parameters\n\n\nHidden Layer Weights (W1):\n\n\n\n\n\n\n\n\n\nx1\nx2\n\n\n\n\nh1\n0.10000\n-0.20000\n\n\nh2\n0.40342\n0.10684\n\n\nh3\n-0.28322\n0.23356\n\n\n\n\n\n\n\nHidden Layer Biases (b1):\n\n\n\n\n\n\n\n\n\nb1\n\n\n\n\nh1\n0.00000\n\n\nh2\n0.00342\n\n\nh3\n0.01678\n\n\n\n\n\n\n\nOutput Layer Weights (W2):\n\n\n\n\n\n\n\n\n\nh1\nh2\nh3\n\n\n\n\ny1\n0.2\n-0.06982\n0.30503\n\n\ny2\n-0.3\n0.51014\n0.10169\n\n\n\n\n\n\n\nOutput Layer Biases (b2):\n\n\n\n\n\n\n\n\n\nb2\n\n\n\n\ny1\n0.0503\n\n\ny2\n0.0169"
  },
  {
    "objectID": "7_monetary_policy/monetary_policy.html",
    "href": "7_monetary_policy/monetary_policy.html",
    "title": "About",
    "section": "",
    "text": "This page is work in progress."
  }
]