---
title: "Asymptotic Normality of OLS"
format: html
execute: 
  warning: false
---

<style>
.quarto-title .title {
  text-align: center;
}
</style>

<br><br>

**Data Generating Process**<br>
Assume you observe $i = 1, ..., N$ observations. For each observation you observe a scalar dependent variable $y_i$ and $k = 1, ..., K$ independent variables $x_i = (x_{i1}, x_{i2},...,x_{ik})$. Assume there is a data generating process that links $y_i$ to $x_i$ described by the linear regression model:

\begin{align*} 
y_i &= \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_k x_{ik} + u_i\\
&= x_i\beta + u_i
\end{align*}

where $\beta = (\beta_1, \beta_2, ..., \beta_k)^T$ is the vector of parameters and $u_i$ the error term. We usually choose $x_{i1} = 1$ for all i to include an unknown intercept $\beta_1$.

For our theoretical analysis we treat the outcome variable $y_i$ and the regressors $x_i$ as random variables. In an application, $y_i$ and $x_i$ have concrete values that are given to us. These concrete values are realizations of the random variables.

Assume the following about the data and the data generating process:

- **A0** $x_i$ and $y_i$ are i.i.d and hence so are the errors $u_i$
- **A1** $\mathbb{E}(x_i^Tu_i) = 0$
- **A2** $\mathbb{E}(x_i^Tx_i)$ exists and rank of $\mathbb{E}(x_i^Tx_i) = K$ i.e. $\mathbb{E}(x_i^Tx_i)$ is invertible
- **A3** $\mathbb{E}(u_i^2x_i^Tx_i)\ \textrm{exists}$

**Theorem: Asymptotic normality of OLS**<br>
Assume the data are i.i.d. draws, and that the linear model and assumptions **A1**, **A2**, and **A3** are satisfied. Let $\Sigma_{xx} = \mathbb{E}(x_i^Tx_i)$ and $S = \mathbb{E}(u_i^2x_i^Tx_i)$. Then as $N \to \infty$ we have that
$$ \sqrt{N}(\hat{\beta} - \beta) \overset{d}\to N(0, \Sigma_{xx}^{-1}S\Sigma_{xx}^{-1}) $$

**Proof:**<br>
We already know that

\begin{align*}
\hat{\beta} - \beta &= \Big(\frac{1}{N}\sum_1^N x_i^T x_{i}\Big)^{-1}\Big(\frac{1}{N}\sum_1^N x_i^T u_i\Big)\\
\sqrt{N}(\hat{\beta} - \beta) &= \Big(\frac{1}{N}\sum_1^N x_i^T x_{i}\Big)^{-1}\Big(\frac{1}{\sqrt{N}}\sum_1^N x_i^T u_i\Big)
\end{align*}

Now we know that
$$ \frac{1}{N}\sum_1^N x_i^T x_{i} \overset{p}{\to} \mathbb{E}[x_i^Tx_i] $$
by the WLLN and that

$$ \frac{1}{\sqrt{N}}\sum_1^N x_i^T u_i = \sqrt{N}\Big(\frac{1}{N}\sum_1^N x_i^T u_i\Big) \overset{d}{\to} N(0, Var(x_i^T u_i)) $$

where 
$$ \text{Var}(x_i^T u_i) = \mathbb{E}[x_i^Tu_i(x_i^Tu_i)^T] = \mathbb{E}[u_i^2x_i^Tx_i] = S $$
by the CLT.

Hence

\begin{align*}
\sqrt{N}(\hat{\beta} - \beta) &\overset{d}{\to} (\mathbb{E}[x_i^Tx_i])^{-1} N(0, S)\\
\sqrt{N}(\hat{\beta} - \beta) &\overset{d}{\to} N(0, (\mathbb{E}[x_i^Tx_i])^{-1}S(\mathbb{E}[x_i^Tx_i])^{-1})\\
\sqrt{N}(\hat{\beta} - \beta) &\overset{d}{\to} N(0, \Sigma_{xx}^{-1}S\Sigma_{xx}^{-1})\\
\end{align*}
