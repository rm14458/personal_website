---
title: "Asymptotic Consistency of OLS"
format: html
execute: 
  warning: false
---

<style>
.quarto-title .title {
  text-align: center;
}
</style>

<br><br>

**Data Generating Process**<br>
Assume you observe $i = 1, ..., N$ observations. For each observation you observe a scalar dependent variable $y_i$ and $k = 1, ..., K$ independent variables $x_i = (x_{i1}, x_{i2},...,x_{ik})$. Assume there is a data generating process that links $y_i$ to $x_i$ described by the linear regression model:

\begin{align*} 
y_i &= \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_k x_{ik} + u_i\\
&= x_i\beta + u_i
\end{align*}

where $\beta = (\beta_1, \beta_2, ..., \beta_k)^T$ is the vector of parameters and $u_i$ the error term. We usually choose $x_{i1} = 1$ for all i to include an unknown intercept $\beta_1$.

For our theoretical analysis we treat the outcome variable $y_i$ and the regressors $x_i$ as random variables. In an application, $y_i$ and $x_i$ have concrete values that are given to us. These concrete values are realizations of the random variables.

Assume the following about the data and the data generating process:

- **A0** $x_i$ and $y_i$ are i.i.d and hence so are the errors $u_i$
- **A1** $\mathbb{E}(x_i^Tu_i) = 0$
- **A2** $\mathbb{E}(x_i^Tx_i)$ exists and rank of $\mathbb{E}(x_i^Tx_i) = K$ i.e. $\mathbb{E}(x_i^Tx_i)$ is invertible

These assumptions are quite weak and will generally hold. Assumption **A1** implies that $\mathbb{E}(u_i) = 0$ and that $\mathbb{E}(u_ix_i) = 0$.

<br>

**Theorem: Consistency of OLS** <br>
Assume the data are i.i.d. draws, and that the linear model and assumptions **A1** and **A2** are satisfied. Then $\hat{\beta} \overset{p}{\to} \beta$ as $n \to \infty$.

**Proof**:
\begin{align}
\hat{\beta} &= \Big(\sum_1^n x_i^T x_{i}\Big)^{-1} \Big(\sum_1^n x_i^T y_i\Big)\nonumber\\
\hat{\beta} &= \Big(\sum_1^n x_i^T x_{i}\Big)^{-1} \Big(\sum_1^n x_i^T x_i\Big)\beta + \Big(\sum_1^n x_i^T x_{i}\Big)^{-1}\Big(\sum_1^n x_i^T u_i\Big)\nonumber\\
\hat{\beta} &= \beta + \Big(\sum_1^n x_i^T x_{i}\Big)^{-1}\Big(\sum_1^n x_i^T u_i\Big)\nonumber\\
\hat{\beta} &= \beta + \Big(\frac{1}{n}\sum_1^n x_i^T x_{i}\Big)^{-1}\Big(\frac{1}{n}\sum_1^n x_i^T u_i\Big)\nonumber\\
\hat{\beta} &\overset{p}{\to} \beta\nonumber \\\nonumber\\\nonumber
\end{align}

Given that 
$$ \frac{1}{n}\sum_1^n x_i^T x_{i} \overset{p}{\to} \mathbb{E}[x_i^Tx_i] $$
$$ \frac{1}{n}\sum_1^n x_i^T u_i \overset{p}{\to} \mathbb{E}[x_i^Tu_i] = 0 $$
by the WLLN.
